{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph Bound / Convergence tool for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigsh\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from variation_utils import calculate_katz, calculate_kmeans_cluster_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse import dok_matrix, csr_matrix, issparse\n",
    "from numba import get_num_threads, njit, prange\n",
    "from numba.typed import Dict, List  # pylint: disable=no-name-in-module\n",
    "\n",
    "\n",
    "def create_partition_matrix(colors):\n",
    "    n = len(colors)\n",
    "    m = np.max(colors) + 1\n",
    "\n",
    "    valid_colors = set(range(m + 1))  # Set of all integers from 0 to max_color\n",
    "    assert set(colors).issubset(valid_colors), \"All colors must be within the range 0 to the maximum color.\"\n",
    "\n",
    "    H = np.zeros((n, m), dtype=int)\n",
    "\n",
    "    for j in range(n):\n",
    "        H[j, colors[j]] = 1\n",
    "\n",
    "    return H\n",
    "\n",
    "@njit\n",
    "def internal_sample_new_adjacency_matrix(N, colors, with_self_loops = False):\n",
    "    n = N.shape[0]\n",
    "\n",
    "    color_to_nodes = Dict()\n",
    "    tmp = Dict()\n",
    "    tmp[0] = 1\n",
    "    color_to_nodes[0] = tmp\n",
    "    del color_to_nodes[0]\n",
    "\n",
    "    for idx, color in enumerate(colors):\n",
    "        if color not in color_to_nodes:\n",
    "            tmp = Dict()\n",
    "            tmp[0] = 1\n",
    "            del tmp[0]\n",
    "            color_to_nodes[color] = tmp\n",
    "        color_to_nodes[color][idx] = 1\n",
    "\n",
    "    row_indices = List([-1])\n",
    "    row_indices.pop()\n",
    "    col_indices = List([-1])\n",
    "    col_indices.pop()\n",
    "\n",
    "    for c, nodes in color_to_nodes.items():\n",
    "        if not nodes:\n",
    "            continue\n",
    "        nodes = set(nodes.keys())\n",
    "\n",
    "        for i in range(n):\n",
    "            num_neighbors = int(N[i][c])  # Colored degree from matrix N\n",
    "            if num_neighbors > 0:\n",
    "                possible_nodes = list(nodes) if with_self_loops else list(nodes - {i})\n",
    "                if num_neighbors <= len(possible_nodes):\n",
    "                    sampled_neighbors = np.random.choice(np.array(possible_nodes), size=num_neighbors, replace=False)\n",
    "                    row_indices.extend([i] * num_neighbors)\n",
    "                    col_indices.extend(sampled_neighbors)\n",
    "\n",
    "    return row_indices, col_indices\n",
    "\n",
    "def calc_edge_probabilities(A, N, H, with_self_loops=False):\n",
    "\n",
    "    color_count = np.sum(H, axis = 0)\n",
    "    if with_self_loops:\n",
    "        denominator = color_count - np.zeros_like(H)\n",
    "    else:\n",
    "        denominator = (color_count-H)\n",
    "\n",
    "    prob = np.zeros_like(N, dtype=float)\n",
    "    safe_divide = denominator != 0\n",
    "    prob[safe_divide] = N[safe_divide] / denominator[safe_divide]\n",
    "\n",
    "    edge_probabilities = np.repeat(prob.flatten(), N.flatten().astype(np.int64))\n",
    "    return edge_probabilities\n",
    "\n",
    "def sample_new_adjacency_matrix(N, colors, with_self_loops = False):\n",
    "    n = N.shape[0]\n",
    "    row_indices, col_indices = internal_sample_new_adjacency_matrix(N, colors, with_self_loops=with_self_loops)\n",
    "    data = np.ones(len(row_indices), dtype=int)\n",
    "    A_prime = csr_matrix((data, (row_indices, col_indices)), shape=(n, n))\n",
    "    return csr_matrix(A_prime)\n",
    "\n",
    "\n",
    "def nest_preserve_colored_out_degree(A, colors, seed = None, with_self_loops = False):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    if not with_self_loops:\n",
    "        if issparse(A):\n",
    "            diag_elements = A.diagonal()\n",
    "        else:\n",
    "            diag_elements = np.diag(A)\n",
    "        assert np.all(diag_elements  == 0), \"The adjacency matrix A should not contain any self-loops.\"\n",
    "    H = create_partition_matrix(colors)\n",
    "    N = A @ H\n",
    "    A_prime = sample_new_adjacency_matrix(N, colors, with_self_loops=with_self_loops)\n",
    "\n",
    "    probs = calc_edge_probabilities(A, N, H, with_self_loops=with_self_loops)\n",
    "\n",
    "    # Check A @ H == A' @ H (algebraic formulation of the entire theory if this fails we ded)\n",
    "    assert np.allclose(N, A_prime @ H)\n",
    "    print(\"Fundamental Algebraic Identity is true!\")\n",
    "    return A_prime, probs\n",
    "\n",
    "def calc_expected_number_of_edges_with_frobenius(A, colors):\n",
    "    P_par = get_P_parallel(colors)\n",
    "    out = np.linalg.norm(A @ P_par, ord=\"fro\")**2\n",
    "\n",
    "    print(\"SLOW; ASSUMES OUT-VAR NEST with_self_loops=TRUE: Using paper equation to calc the expected number of edges\", out)\n",
    "    return out\n",
    "\n",
    "def get_P_parallel(colors):\n",
    "    n = len(colors)\n",
    "    unique_colors = np.unique(colors)\n",
    "    m = len(unique_colors)\n",
    "    color_to_idx = {color: idx for idx, color in enumerate(unique_colors)}\n",
    "\n",
    "    H = np.zeros((n, m), dtype=int)\n",
    "\n",
    "    for j in range(n):\n",
    "        H[j, color_to_idx[colors[j]]] = 1\n",
    "\n",
    "    HtH_inv = np.diag(1 / np.diag((H.T @ H)))\n",
    "    P_par = H @ HtH_inv @ H.T\n",
    "    return P_par\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "colors = np.array([1, 2, 2, 3, 3, 3])\n",
    "Atest = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [1, 0, 0, 1, 1, 0],\n",
    "    [1, 1, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 0]\n",
    "])\n",
    "\n",
    "A_prime, edge_probs = nest_preserve_colored_out_degree(Atest, colors, with_self_loops=False)\n",
    "\n",
    "print(\"New Adjacency Matrix A':\")\n",
    "print(A_prime.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectral norm != spectral radius for non symmetric graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anonymigraph.anonymization._external.nest_model._rewire import _rewire\n",
    "import optimal1dclustering\n",
    "import math\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.sparse.linalg import svds\n",
    "from anonymigraph.anonymization._external.nest_model.fast_wl import WL_fast\n",
    "\n",
    "def calc_min_cluster_size(arr):\n",
    "    arr = np.array(arr)\n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    min_size = counts.min()\n",
    "    return min_size\n",
    "\n",
    "\n",
    "def get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.1, beta = 1,  printing = False, max_k = float(\"Inf\"), max_iter=None, tol=None, random_seed=44, with_self_loops=False):\n",
    "    A_G = nx.adjacency_matrix(G).astype(np.float64)  # Get the adjacency matrix as a sparse matrix\n",
    "    number_of_edges = G.number_of_edges()\n",
    "    _, s, _ = svds(A_G, k=1)\n",
    "    spectral_norm_G = s[0]\n",
    "    eigenvalues, eigenvectors = eigs(A_G.astype(np.float64), k=1, which='LM')  # 'LM': Largest Magnitude, tol is tolerance\n",
    "    spectral_radius_G = np.abs(eigenvalues).max()\n",
    "\n",
    "    print(\"Spectral radius:\", spectral_radius_G, \"Spectral norm:\", spectral_norm_G)\n",
    "    print(f\"Spectral Radius: {spectral_radius_G}, alpha_max = {1/spectral_radius_G}\")\n",
    "\n",
    "\n",
    "    assert alpha < 1/spectral_radius_G, f\"for katz to converge alpha needs to be smaller than 1/spectral_radius ({1/spectral_radius_G})\"\n",
    "\n",
    "    katz_cent = calculate_katz(A_G, alpha = alpha, beta=beta, num_iters=max_iter, tol=tol)\n",
    "    katz_cent_norm = np.linalg.norm(katz_cent)\n",
    "    katz_cent_norm_bound = beta * np.sqrt(A_G.shape[0]) /(1 - alpha * spectral_norm_G) # 1/(1 - alpha * spectral_norm) * norm(beta)\n",
    "    print(\"katz_cent_norm < katz_cent_norm_bound\", katz_cent_norm, \"< \", katz_cent_norm_bound)\n",
    "\n",
    "    data_dict = {}\n",
    "    for k in range(1, min(G.number_of_nodes()//min_cluster_size, max_k)):\n",
    "        print(f\"{k}: \")\n",
    "        results = {}\n",
    "        mode = 2\n",
    "        colors, centroids = optimal1dclustering.cluster(\n",
    "            katz_cent, k, mode=mode, min_cluster_size=min_cluster_size\n",
    "        )\n",
    "        colors = np.array(colors)\n",
    "        clusterLoss = calculate_kmeans_cluster_loss(katz_cent, colors, centroids, mode = mode)\n",
    "\n",
    "\n",
    "\n",
    "        # Get graph spectrum statistics\n",
    "        A_Ga, edge_probs = nest_preserve_colored_out_degree(A_G, colors, seed = random_seed, with_self_loops=with_self_loops)\n",
    "        A_Ga = A_Ga.astype(np.float64)\n",
    "\n",
    "        assert A_Ga.nnz == A_G.nnz # Check if A_G and A_Ga have same number of edges\n",
    "        number_overlapping_edges = A_G.multiply(A_Ga).sum()\n",
    "\n",
    "        _, s, _ = svds(A_Ga, k=1)\n",
    "        spectral_norm_Ga = s[0]\n",
    "\n",
    "        print(f\"Sampled A_Ga spectral norm {spectral_norm_Ga}\")\n",
    "\n",
    "        katz_cent_Ga = calculate_katz(A_Ga, alpha = alpha, beta=beta, num_iters=max_iter, tol=tol)\n",
    "\n",
    "        katz_l2_diff = np.linalg.norm(katz_cent_Ga - katz_cent)\n",
    "        katz_l2_diff_bound_neumann = (alpha * (spectral_radius_G + spectral_norm_Ga) * clusterLoss) / (1 - alpha * spectral_norm_Ga)\n",
    "\n",
    "        P_par = get_P_parallel(colors)\n",
    "        expected_katz_cent_Ga = calculate_katz(A_G @ P_par, alpha = alpha, beta=beta, num_iters=max_iter, tol=tol)\n",
    "        expected_katz_l2_diff = np.linalg.norm(expected_katz_cent_Ga - katz_cent)\n",
    "\n",
    "        results['katz_l2_diff'] = katz_l2_diff\n",
    "        results['expected_katz_l2_diff'] = expected_katz_l2_diff\n",
    "        results['katz_l2_diff_bound_neumann'] = katz_l2_diff_bound_neumann\n",
    "        results['Clustering Loss'] = clusterLoss\n",
    "        results['percentage_overlapping_edges'] = number_overlapping_edges/number_of_edges\n",
    "        results['expected_percentage_overlapping_edges'] = calc_expected_number_of_edges_with_frobenius(A_G, colors)/number_of_edges #np.sum(edge_probs)/number_of_edges\n",
    "        results['edge_probs'] = edge_probs\n",
    "\n",
    "        data_dict[k] = results\n",
    "\n",
    "    edges = np.array(G.edges(), dtype=np.uint32)\n",
    "    all_depth_colors = WL_fast(edges, labels=None, max_iter=None)\n",
    "\n",
    "    print(f\"Calculating Performance of Original Nest, with {len(all_depth_colors)} depths\")\n",
    "    original_nest = {}\n",
    "    for i in range(len(all_depth_colors)):\n",
    "\n",
    "        print(f\"{i}: Calculating Nest at depth {i}\")\n",
    "\n",
    "        colors = all_depth_colors[i]\n",
    "\n",
    "        k_approx = len(np.unique(colors))\n",
    "        if k_approx > max_k:\n",
    "            continue\n",
    "\n",
    "        A_Ga, edge_probs = nest_preserve_colored_out_degree(A_G, colors, seed = random_seed, with_self_loops=with_self_loops)\n",
    "        A_Ga = A_Ga.astype(np.float64)\n",
    "\n",
    "        results = {}\n",
    "        katz_cent_Ga = calculate_katz(A_Ga, alpha = alpha, beta=beta, num_iters=max_iter, tol=tol)\n",
    "        katz_l2_diff = np.linalg.norm(katz_cent_Ga - katz_cent)\n",
    "\n",
    "        results['katz_l2_diff'] = katz_l2_diff\n",
    "\n",
    "        original_nest[k_approx] = results\n",
    "\n",
    "    return {\"data_dict\": data_dict, \"original_nest\":original_nest, \"katz_cent_norm\":katz_cent_norm, \"lam_G\":spectral_radius_G, \"katz_alpha\":alpha, \"katz_cent_norm_bound\": katz_cent_norm_bound}\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "def plot_convergence_data(data, title, plot_nest=True):\n",
    "    data_dict = data[\"data_dict\"]\n",
    "\n",
    "    # Create traces for each metric\n",
    "    traces = []\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                                    y=[data_dict[k]['Clustering Loss'] for k in data_dict.keys()],\n",
    "                                    mode='lines',#+markers',\n",
    "                                    name=r'$\\ell$'))\n",
    "    # Add a constant line at y = 5\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                            y=[data[\"katz_cent_norm\"],]*len(data_dict),  # This creates a list of 5's of the same length as the data_dict keys\n",
    "                            mode='lines',\n",
    "                            name='Norm of Katz Centrality for G'))\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                            y=[data[\"katz_cent_norm_bound\"],]*len(data_dict),  # This creates a list of 5's of the same length as the data_dict keys\n",
    "                            mode='lines',\n",
    "                            name='Bound for Norm of Katz Centrality for G'))\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                                    y=[data_dict[k]['katz_l2_diff'] for k in data_dict.keys()],\n",
    "                                    mode='lines',#+markers',\n",
    "                                    name=r'Katz L2 Norm Diff (Directed Katz Variation NeSt)'))\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                                    y=[data_dict[k]['expected_katz_l2_diff'] for k in data_dict.keys()],\n",
    "                                    mode='lines',#+markers',\n",
    "                                    name=r'Expected Katz L2 Norm Diff (Directed Katz Variation NeSt)'))\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                                    y=[data_dict[k]['katz_l2_diff_bound_neumann'] for k in data_dict.keys()],\n",
    "                                    mode='lines',#+markers',\n",
    "                                    name=r'Katz L2 Norm Diff Bound (Neumann)'))\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                                    y=[data_dict[k]['percentage_overlapping_edges'] for k in data_dict.keys()],\n",
    "                                    mode='lines',#+markers',\n",
    "                                    name='Percentage of overlapping edges between G and Ga',\n",
    "                                    yaxis='y2'))\n",
    "    traces.append(go.Scatter(x=list(data_dict.keys()),\n",
    "                                    y=[data_dict[k]['expected_percentage_overlapping_edges'] for k in data_dict.keys()],\n",
    "                                    mode='lines',#+markers',\n",
    "                                    name='Expected percentage of overlapping edges between G and Ga',\n",
    "                                    yaxis='y2'))\n",
    "    if plot_nest:\n",
    "        original_nest = data[\"original_nest\"]\n",
    "\n",
    "        traces.append(go.Scatter(x=list(original_nest.keys()),\n",
    "                y=[original_nest[k]['katz_l2_diff'] for k in original_nest.keys()],\n",
    "                mode='markers',\n",
    "                name=\"Original Nest k=len(wl-colors)\",\n",
    "                marker=dict(symbol='x', size=10)))\n",
    "\n",
    "\n",
    "    layout_loglog = go.Layout(title=title,\n",
    "                            xaxis_title='k',\n",
    "                            yaxis_title='Metric Values (Log Scale)',\n",
    "                            yaxis_type='log',  # Set y-axis to log scale\n",
    "                            # Add a second y-axis to the layout\n",
    "                            yaxis=dict(\n",
    "                                exponentformat='e',  # Use scientific notation for the primary y-axis\n",
    "                            ),\n",
    "                            yaxis2=dict(title='Edge Percentages',\n",
    "                                        titlefont=dict(color='rgba(148, 103, 189, 1)'),\n",
    "                                        tickfont=dict(color='rgba(148, 103, 189, 1)'),\n",
    "                                        overlaying='y',  # This places the second y-axis on top of the first\n",
    "                                        side='right',  # This places the second y-axis on the right\n",
    "                                        type='linear',\n",
    "                                        range=[0, 1],),  # Set the second y-axis to linear scale\n",
    "                            hovermode='closest',\n",
    "                            height=900,\n",
    "                            legend=dict(\n",
    "                                    orientation=\"h\",\n",
    "                                    x=0.5,\n",
    "                                    y=-0.1,\n",
    "                                    xanchor=\"center\",\n",
    "                                    yanchor=\"top\"\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Add the new trace to the figure\n",
    "    fig = go.Figure(data=traces, layout=layout_loglog)\n",
    "    fig.layout.template = 'simple_white+gridon' # 'presentation'\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    only_plot_every_xth = 10\n",
    "    last_key = max(data_dict.keys())\n",
    "    subsampled_data_dict = {key: data_dict[key] for key in data_dict if key % only_plot_every_xth == 0 or key == last_key or key == 1}\n",
    "\n",
    "\n",
    "    initial_key = 1\n",
    "    fig = go.Figure(data=[go.Histogram(x=subsampled_data_dict[initial_key][\"edge_probs\"],\n",
    "                                        xbins=dict(\n",
    "                                            size = 0.01\n",
    "                                        ),\n",
    "                                        histnorm='probability',\n",
    "                                        )])\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title_text='Histogram of probabilities of an particular edge being present in the anonymized graph given it was present in the original graph', # title of the plot\n",
    "        xaxis_title_text='Probability of an particular edge being present in the anonymized graph given it was present in the original graph', # xaxis label\n",
    "        yaxis_title_text='Proportion of edges with the given conditional probability', # yaxis label\n",
    "        template='simple_white+gridon',\n",
    "        xaxis=dict(\n",
    "            range=[0, 1]  # setting the range for the x-axis\n",
    "        ),\n",
    "        sliders=[{\n",
    "            \"steps\": [{\n",
    "                \"label\": str(key),\n",
    "                \"method\": \"update\",\n",
    "                \"args\": [{\n",
    "                    \"x\": [subsampled_data_dict[key][\"edge_probs\"]],\n",
    "                    \"name\": f\"Key {key}\"\n",
    "                }, {\"title\": f\"Histogram of probabilities for key {key}\"}]\n",
    "            } for key in subsampled_data_dict.keys()],\n",
    "            \"active\": list(data_dict.keys()).index(initial_key),\n",
    "            \"currentvalue\": {\n",
    "                \"prefix\": \"Currently viewing: k=\",\n",
    "                \"visible\": True\n",
    "            },\n",
    "            \"transition\": {\"duration\": 300},\n",
    "            \"pad\": {\"t\": 50}  # Optional: add padding to the top of the slider\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erdos Renyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Graph\n",
    "n = 400\n",
    "p = 3/n\n",
    "\n",
    "G = nx.erdos_renyi_graph(n, p, directed=True)\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.04, beta=1, max_k=400, with_self_loops=True)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Erdos Renyi Graph: n={n}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Graph\n",
    "n = 400\n",
    "p = 3/n\n",
    "\n",
    "G = nx.erdos_renyi_graph(n, p, directed=True)\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.3, beta=1, max_k=400, with_self_loops=True)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Erdos Renyi Graph: n={n}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Graph\n",
    "n = 400\n",
    "p = 8/400\n",
    "\n",
    "G = nx.erdos_renyi_graph(n, p, directed=True)\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.04, beta=1, max_k=400, with_self_loops=True)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Erdos Renyi Graph: n={n}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "p = 20/n\n",
    "\n",
    "G = nx.erdos_renyi_graph(n, p, directed=True)\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.04, beta=1, max_k=400, with_self_loops=True)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Erdos Renyi Graph: n={n}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 150\n",
    "p = 90/n\n",
    "\n",
    "G = nx.erdos_renyi_graph(n, p, directed=True)\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.01, beta=1, max_k=150, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Erdos Renyi Graph: n={n}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "n = 600\n",
    "\n",
    "G = nx.scale_free_graph(n, seed=8)\n",
    "G = nx.DiGraph([(u, v) for u, v in G.edges() if u != v])\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.05, beta=1, max_k=600, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Scale-Free Graph: n={n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Graph\n",
    "n = 400\n",
    "tau1 = 3\n",
    "tau2 = 1.5\n",
    "mu = 0.1\n",
    "G = nx.LFR_benchmark_graph(\n",
    "    n, tau1, tau2, mu, average_degree=10, min_community=80, seed=10\n",
    ")\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "G = G.to_directed()\n",
    "print(G)\n",
    "\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size = 1, alpha=0.02, beta=1, max_k=400, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | LFR Graph: n={n}, τ1={tau1}, τ2={tau2}, μ={mu}, AvgDeg=10, MinComm=80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 200  # number of nodes\n",
    "\n",
    "np.random.seed(42)\n",
    "degree_sequence = np.random.zipf(a=1.2, size=n)\n",
    "degree_sequence = [d for d in degree_sequence if 0 < d < n]\n",
    "if sum(degree_sequence) % 2 == 1:\n",
    "    degree_sequence[-1] += 1\n",
    "print(degree_sequence)\n",
    "# Create a directed graph from a degree sequence\n",
    "G = nx.directed_configuration_model(degree_sequence, degree_sequence, seed=42)\n",
    "G = nx.DiGraph([(u, v) for u, v in G.edges() if u != v])\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "# Assuming 'get_convergence_data_of_nest' and 'plot_convergence_data' are defined\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size=1, alpha=0.02, beta=1, max_k=n, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Graph with Zipf Degrees: n={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 200  # number of nodes\n",
    "\n",
    "np.random.seed(42)\n",
    "degree_sequence = np.random.zipf(a=1.9, size=n)\n",
    "degree_sequence = [d for d in degree_sequence if 0 < d < n]\n",
    "if sum(degree_sequence) % 2 == 1:\n",
    "    degree_sequence[-1] += 1\n",
    "print(degree_sequence)\n",
    "# Create a directed graph from a degree sequence\n",
    "G = nx.directed_configuration_model(degree_sequence, degree_sequence, seed=42)\n",
    "G = nx.DiGraph([(u, v) for u, v in G.edges() if u != v])\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "# Assuming 'get_convergence_data_of_nest' and 'plot_convergence_data' are defined\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size=1, alpha=0.03, beta=1, max_k=n, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Graph with Zipf Degrees: n={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 200  # number of nodes\n",
    "\n",
    "np.random.seed(42)\n",
    "degree_sequence = np.random.zipf(a=1.7, size=n)\n",
    "degree_sequence = [d for d in degree_sequence if 0 < d < n]\n",
    "if sum(degree_sequence) % 2 == 1:\n",
    "    degree_sequence[-1] += 1\n",
    "print(degree_sequence)\n",
    "# Create a directed graph from a degree sequence\n",
    "G = nx.directed_configuration_model(degree_sequence, degree_sequence, seed=42)\n",
    "G = nx.DiGraph([(u, v) for u, v in G.edges() if u != v])\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "# Assuming 'get_convergence_data_of_nest' and 'plot_convergence_data' are defined\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size=1, alpha=0.03, beta=1, max_k=n, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Graph with Zipf Degrees: n={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 300  # number of nodes\n",
    "\n",
    "# Create a directed graph from a degree sequence\n",
    "G = nx.random_k_out_graph(n, 10, 1, self_loops=False, seed=555)\n",
    "G = nx.DiGraph([(u, v) for u, v in G.edges() if u != v])\n",
    "\n",
    "print(G, f\"number of selfloops {nx.number_of_selfloops(G)}\")\n",
    "\n",
    "# Assuming 'get_convergence_data_of_nest' and 'plot_convergence_data' are defined\n",
    "data_dict = get_convergence_data_of_nest(G, min_cluster_size=1, alpha=0.03, beta=1, max_k=n, with_self_loops=False)\n",
    "plot_convergence_data(data_dict, f\"spectral_radius*alpha = {data_dict[\"katz_alpha\"]*data_dict[\"lam_G\"]:.3f} | Directed Graph with Zipf Degrees: n={n}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

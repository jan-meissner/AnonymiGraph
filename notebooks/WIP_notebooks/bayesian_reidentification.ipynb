{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm seems to be broken in SecGraph too on detailed review. Not straightforward to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.linalg import pinv\n",
    "from anonymigraph.anonymization.random_edge_add_del import RandomEdgeAddDelAnonymizer\n",
    "\n",
    "class CachedShortestPath:\n",
    "    def __init__(self, G):\n",
    "        self.cache = {}\n",
    "        self.G = G\n",
    "\n",
    "    def distance(self, source, target):\n",
    "        if source not in self.cache:\n",
    "            self.cache[source] = nx.single_source_shortest_path_length(self.G, source)\n",
    "\n",
    "        return self.cache[source].get(target, np.Inf)\n",
    "\n",
    "class CachedResistanceDistance:\n",
    "    def __init__(self, G):\n",
    "        self.G = G\n",
    "        self.L_pinv = None  # Store the pseudoinverse of the adjusted Laplacian\n",
    "        self.cache = {}  # Cache for resistance distances\n",
    "\n",
    "    def compute_laplacian_pseudoinverse(self):\n",
    "        L = nx.laplacian_matrix(self.G).toarray()  # Get the Laplacian matrix as a NumPy array\n",
    "        n = len(self.G.nodes())  # Number of nodes in the graph\n",
    "        adjusted_L = L + 1/n\n",
    "        self.L_pinv = pinv(adjusted_L)  # Compute and store the pseudoinverse of the adjusted Laplacian\n",
    "\n",
    "    def distance(self, source, target):\n",
    "        # Ensure the Laplacian pseudoinverse is computed\n",
    "        if self.L_pinv is None:\n",
    "            self.compute_laplacian_pseudoinverse()\n",
    "\n",
    "        # Check if the distance is already cached\n",
    "        if (source, target) in self.cache:\n",
    "            return self.cache[(source, target)]\n",
    "        elif (target, source) in self.cache:  # Resistance distance is symmetric\n",
    "            return self.cache[(target, source)]\n",
    "\n",
    "        # Compute resistance distance using the adjusted Laplacian pseudoinverse\n",
    "        R_ij = self.L_pinv[source, source] + self.L_pinv[target, target] - 2 * self.L_pinv[source, target]\n",
    "        self.cache[(source, target)] = R_ij\n",
    "        return R_ij\n",
    "\n",
    "\n",
    "\n",
    "def bayesian_using_cos_sim(G, Ga):\n",
    "    if G.number_of_nodes() != Ga.number_of_nodes():\n",
    "        raise ValueError(\"Graph node sizes do not match\")\n",
    "\n",
    "    sorted_G_nodes = sorted(G.nodes(), key=lambda x: G.degree(x), reverse=True)\n",
    "    sorted_Ga_nodes = sorted(Ga.nodes(), key=lambda x: Ga.degree(x), reverse=True)\n",
    "\n",
    "    n = 2\n",
    "    g1_matched, g2_matched = [], []\n",
    "    g1_anchor, g2_anchor = [], []\n",
    "\n",
    "    G_lengths_cache = CachedShortestPath(G)\n",
    "    Ga_lengths_cache = CachedShortestPath(G)\n",
    "\n",
    "    while len(g1_matched) < G.number_of_nodes():\n",
    "        g1_nodes_consider = sorted_G_nodes[:n]\n",
    "        g2_nodes_consider = sorted_Ga_nodes[:n]\n",
    "\n",
    "        print(\"cand g1\", g1_nodes_consider)\n",
    "        print(\"cand g2\", g2_nodes_consider)\n",
    "\n",
    "        g1_cand_finger = get_candidate_fingerprint(g1_anchor, g1_nodes_consider, G, G_lengths_cache)\n",
    "        g2_cand_finger = get_candidate_fingerprint(g2_anchor, g2_nodes_consider, Ga, Ga_lengths_cache)\n",
    "\n",
    "        #score_matrix  = normalize(compute_score_matrix(g1_nodes_consider, g2_nodes_consider, g1_cand_finger, g2_cand_finger))\n",
    "        #score_matrix = normalize(vectorized_compute_score_matrix_euclidean(g1_nodes_consider, g2_nodes_consider, g1_cand_finger, g2_cand_finger))\n",
    "        score_matrix = vectorized_compute_score_matrix_euclidean(g1_nodes_consider, g2_nodes_consider, g1_cand_finger, g2_cand_finger)\n",
    "        row_ind, col_ind = linear_sum_assignment(score_matrix) # outputs two lists that for the same index indicate the matches\n",
    "        print(\"row_ind\", row_ind)\n",
    "        print(\"col_ind\", col_ind)\n",
    "\n",
    "        matches = [(row_ind[i], col_ind[i], score_matrix[row_ind[i], col_ind[i]]) for i in range(len(row_ind))]\n",
    "        matches.sort(key=lambda x: x[2], reverse=False)\n",
    "\n",
    "        g1_matched, g2_matched = [], []\n",
    "        g1_anchor, g2_anchor = [], []\n",
    "        for i, (g1_idx, g2_idx, score) in enumerate(matches):\n",
    "            #print(score, g1_nodes_consider[g1_idx], g2_nodes_consider[g2_idx])\n",
    "            g1_matched.append(g1_nodes_consider[g1_idx])\n",
    "            g2_matched.append(g2_nodes_consider[g2_idx])\n",
    "\n",
    "            if i < len(matches) // 2:\n",
    "                g1_anchor.append(g1_nodes_consider[g1_idx])\n",
    "                g2_anchor.append(g2_nodes_consider[g2_idx])\n",
    "\n",
    "        n = min(n * 2, G.number_of_nodes())\n",
    "\n",
    "\n",
    "    return dict(zip(g1_matched, g2_matched))\n",
    "\n",
    "def normalize(matrix):\n",
    "    outer_sums = np.sqrt(np.outer(matrix.sum(axis=1), matrix.sum(axis=0)))\n",
    "    normalized_matrix = matrix / outer_sums\n",
    "    return normalized_matrix\n",
    "\n",
    "def get_candidate_fingerprint(anchors, nodes_consider, G, G_lengths_cache):\n",
    "    fingerprints = {}\n",
    "    for node in nodes_consider:\n",
    "        fingerprint = [G.degree(node)]  # Degree as the first element\n",
    "        for anchor in anchors:\n",
    "            #Cache does same as nx.shortest_path_length(G, source=node, target=anchor) but efficient\n",
    "            fingerprint.append(G_lengths_cache.distance(node, anchor))\n",
    "        fingerprints[node] = np.array(fingerprint, dtype=np.float64)\n",
    "    return fingerprints\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vectorized_compute_score_matrix_euclidean(g1_nodes, g2_nodes, g1_finger, g2_finger):\n",
    "    g1_matrix = np.array([g1_finger[node] for node in g1_nodes])\n",
    "    g2_matrix = np.array([g2_finger[node] for node in g2_nodes])\n",
    "\n",
    "    combined_matrix = np.concatenate([g1_matrix, g2_matrix], axis=0)\n",
    "\n",
    "    # Step 2: Compute the mean and standard deviation of the combined matrix\n",
    "    combined_mean = np.mean(combined_matrix, axis=0)\n",
    "    combined_std = np.std(combined_matrix, axis=0)\n",
    "    combined_std = np.where(combined_std == 0, 1, combined_std)\n",
    "    #print(\"means\", combined_mean)\n",
    "    #print(\"stds\", combined_std)\n",
    "\n",
    "\n",
    "    #print(combined_std.shape)\n",
    "    #print(combined_std[0])\n",
    "    #combined_std[0] /= 10\n",
    "\n",
    "    # Step 3: Normalize g1_matrix and g2_matrix using combined mean and std\n",
    "    g1_normed = (g1_matrix - combined_mean) / combined_std\n",
    "    g2_normed = (g2_matrix - combined_mean) / combined_std\n",
    "\n",
    "    # Compute the L2 norm (Euclidean distance) between each pair of vectors\n",
    "    # Expanding the Euclidean distance formula: ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a.b\n",
    "    g1_sq_norms = np.sum(g1_normed**2, axis=1, keepdims=True)  # ||a||^2 for each vector in g1\n",
    "    g2_sq_norms = np.sum(g2_normed**2, axis=1, keepdims=True).T  # ||b||^2 for each vector in g2, transposed to align with matrix multiplication rules\n",
    "\n",
    "    # Compute squared Euclidean distance\n",
    "    squared_euclidean_distance = g1_sq_norms + g2_sq_norms - 2 * g1_normed @ g2_normed.T\n",
    "    #print(\"g1_normed?\", g1_normed)\n",
    "    #print(\"g2_normed\", g2_normed)\n",
    "    #print(\"euclidian_dist\", squared_euclidean_distance)\n",
    "    # euclidean_distance = np.sqrt(np.maximum(squared_euclidean_distance, 0))\n",
    "\n",
    "    return squared_euclidean_distance\n",
    "\n",
    "\n",
    "def vectorized_compute_score_matrix_cosine(g1_nodes, g2_nodes, g1_finger, g2_finger):\n",
    "    g1_matrix = np.array([g1_finger[node] for node in g1_nodes])\n",
    "    g2_matrix = np.array([g2_finger[node] for node in g2_nodes])\n",
    "\n",
    "    g1_norms = np.linalg.norm(g1_matrix, axis=1, keepdims=True)\n",
    "    g2_norms = np.linalg.norm(g2_matrix, axis=1, keepdims=True)\n",
    "    g1_norms[g1_norms == 0] = 1\n",
    "    g2_norms[g2_norms == 0] = 1\n",
    "\n",
    "    dot_product = g1_matrix @ g2_matrix.T\n",
    "    cosine_similarity = dot_product / (g1_norms @ g2_norms.T)\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def compute_score_matrix(g1_nodes, g2_nodes, g1_finger, g2_finger):\n",
    "    matrix_size = max(len(g1_nodes), len(g2_nodes))\n",
    "    score_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    for i, g1_node in enumerate(g1_nodes):\n",
    "        for j, g2_node in enumerate(g2_nodes):\n",
    "            score_matrix[i, j] = cosine_similarity(g1_finger[g1_node], g2_finger[g2_node])\n",
    "    return score_matrix\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        similarity = dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "def sample_edges(G, s, seed=10):\n",
    "    random.seed(seed)\n",
    "    G1 = nx.Graph()\n",
    "    G2 = nx.Graph()\n",
    "    G1.add_nodes_from(G.nodes())\n",
    "    G2.add_nodes_from(G.nodes())\n",
    "\n",
    "    for edge in G.edges():\n",
    "        if random.random() < s:\n",
    "            G1.add_edge(*edge)\n",
    "        if random.random() < s:\n",
    "            G2.add_edge(*edge)\n",
    "\n",
    "    return G1, G2\n",
    "\n",
    "def shuffle_and_sort_graph(Ga):\n",
    "    \"\"\"\n",
    "    Shuffles the labels of the nodes in the input graph Ga, and then sorts the nodes,\n",
    "    creating a new graph with the nodes sorted and edges preserved according to the\n",
    "    shuffled labels.\n",
    "\n",
    "    Parameters:\n",
    "    Ga (nx.Graph): The original graph to be shuffled and sorted.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: A new graph with nodes sorted after being shuffled.\n",
    "    \"\"\"\n",
    "    original_labels = list(Ga.nodes())\n",
    "    random_labels = original_labels.copy()\n",
    "    random.shuffle(random_labels)\n",
    "    label_mapping = dict(zip(original_labels, random_labels))\n",
    "    Ga_relabelled = nx.relabel_nodes(Ga, label_mapping)\n",
    "    Ga_sorted = nx.Graph()\n",
    "    sorted_nodes = sorted(Ga_relabelled.nodes(data=True))\n",
    "    Ga_sorted.add_nodes_from(sorted_nodes)\n",
    "    Ga_sorted.add_edges_from(Ga_relabelled.edges(data=True))\n",
    "    return Ga_sorted, label_mapping\n",
    "\n",
    "import networkx as nx\n",
    "import cProfile\n",
    "\n",
    "def main():\n",
    "    G = nx.barabasi_albert_graph(30, 4, seed=10)\n",
    "    #G = nx.erdos_renyi_graph(1000, 12/1000)\n",
    "    degrees = [degree for _, degree in G.degree()]\n",
    "    print(f\"Max Degree: {max(degrees)}, Mean Degree: {sum(degrees) / len(degrees):.2f}\")\n",
    "\n",
    "    #m = int(1/100*G.number_of_edges())\n",
    "    #print(G, m)\n",
    "    #Ga = RandomEdgeAddDelAnonymizer(m = m).anonymize(G, random_seed = 10)\n",
    "\n",
    "    # Generate the sampled graphs G1 and G2\n",
    "    s_square = 2\n",
    "    G, Ga = sample_edges(G, np.sqrt(s_square), seed = 10)\n",
    "    #print(nx.is_isomorphic(G, Ga))\n",
    "\n",
    "    Ga, re_id_truth = shuffle_and_sort_graph(Ga)\n",
    "\n",
    "    # Step 6: Output the relabeled graph and the mapping of labels.\n",
    "    print(\"Ground Truh Re-Identification:\", re_id_truth)\n",
    "\n",
    "    match_bayesian = bayesian_using_cos_sim(G, Ga)\n",
    "    print(\"accuracy reidentification bayesian\", sum(match_bayesian[g1_node] == re_id_truth[g1_node] for g1_node, _ in re_id_truth.items())/len(re_id_truth))\n",
    "\n",
    "    k = 5\n",
    "    top_k_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    print(f\"accuracy reidentification bayesian top {k}\", sum(match_bayesian[g_node] == re_id_truth[g_node] for g_node, _ in top_k_nodes)/k)\n",
    "\n",
    "    ####\n",
    "    sorted_G_nodes = sorted(G.nodes(), key=lambda x: G.degree(x), reverse=True)\n",
    "    sorted_Ga_nodes = sorted(Ga.nodes(), key=lambda x: Ga.degree(x), reverse=True)\n",
    "    match_degree = dict(zip(sorted_G_nodes, sorted_Ga_nodes))\n",
    "    print(\"accuracy reidentification degree\", sum(match_degree[g1_node] == re_id_truth[g1_node] for g1_node, _ in re_id_truth.items())/len(re_id_truth))\n",
    "\n",
    "main()\n",
    "#cProfile.run('main()', 'profile_output.prof')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
